# Arabic-GEC
Neural-based automatic Arabic Grammar Error Correction (AGEC) model based on sequence-to-sequence multi-heads attentions Transformer. Initially, we introduce a an unsupervised method to generate a large-scale synthetic dataset based on confusion function to increase the amount of training set. The standard seq2seq Transformer is equipped with capsule network to aggregate linguistic features cross layers dynamically we also added a regularization term in the training objective using Kullback-Leibler divergence to overcome to improve the agreement between R2L and L2R models.
